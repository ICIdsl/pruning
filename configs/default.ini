[dataset]
Dataset: cifar10
Dataset_Location: /data

[cnn]
Architecture: alexnet
Depth: 29 
Cardinality: 8
Widen_Factor: 4 
Growth_Rate: 12 
Compression_Rate: 2

[training_hyperparameters]
Print_Only: True
Total_Epochs: 29
Train_Batch: 32 
Test_Batch: 32
Learning_Rate: 0.001
Dropout_Ratio: 0.5 
Gamma: 0.1 
Momentum: 0.9 
Weight_Decay: 5e-4
Momentum_Schedule: 
LR_Schedule: 
Train_Val_Split: 0.8

[pruning_hyperparameters]
Get_Gops: False
Sub_Classes:

Finetune: False
This_Layer_Up: 0
Pruning_Perc: 80
Prune_Weights: False
Prune_Filters: True
Metric: weights

FBS_Pruning: False
FBS_Finetune: False
Unpruned_Ratio: 0.2
Unpruned_LB: 0.1
Batch_Lim: -1

Iterative_Pruning_Increment: 10
Iterative_Pruning_Epochs: 15

[pytorch_parameters]
Manual_Seed: -1
Data_Loading_Workers: 4 
GPU_ID: 1
Checkpoint_Path: /home/ar4414/pytorch_projs/pytorch_training/src/ar4414/pruning/logs/
Test_Name: googlenet_cifar100_structured_activations
Pretrained: /home/ar4414/pytorch_training/src/ar4414/pruning/logs/googlenet/googlenet_cifar100_pruning_aware_baseline/orig/299-model.pth.tar
# Pretrained: /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/alexnet_cifar100_pruning_aware_baseline/orig/149-model.pth.tar
Resume: False
Branch: False
Evaluate: False
Tee_Printing: None
