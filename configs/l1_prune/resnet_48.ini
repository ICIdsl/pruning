[dataset]
dataset = cifar100
dataset_location = /data

[cnn]
architecture = resnet
depth = 20
cardinality = 8
widen_factor = 4
growth_rate = 12
compression_rate = 2

[training_hyperparameters]
print_only = False
total_epochs = 150
train_batch = 128
test_batch = 128
learning_rate = 0.1
dropout_ratio = 0.5
gamma = 0.1
momentum = 0.9
weight_decay = 5e-4
momentum_schedule = 
lr_schedule = 0 0.001 5 0.001 15 -1 25 -1
train_val_split = 0.8

[pruning_hyperparameters]
sub_name = entire_dataset
sub_classes = 
get_gops = False
inference_gops = False
logs = /home/ar4414/pytorch_training/src/ar4414/pruning/plotting/logs.json
change_in_rank = False
prune_filters = True
finetune = True
static = True
retrain = False
pruning_perc = 55
finetune_budget = 30
prune_after = 5
channels_pruned = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/resnet/cifar100/entire_dataset/l1_prune/pp_95/2019-11-20-00-17-54/orig/pruned_channels.json
this_layer_up = 0
prune_weights = False
metric = weights
fbs_pruning = False
fbs_finetune = False
unpruned_ratio = 1.0
unpruned_lb = 0.1
batch_lim = -1
iterative_pruning_increment = 10
iterative_pruning_epochs = 15

[entropy_hyperparameters]
entropy = False
layers = 
channels = -1
num_batches = 320
entropy_global_pruning = False

[pytorch_parameters]
manual_seed = -1
data_loading_workers = 4
gpu_id = 0
checkpoint_path = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/resnet/cifar100/entire_dataset/l1_prune
test_name = pp_55
pretrained = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/resnet20/cifar100/baseline/2019-08-27-11-58-14/orig/118-model.pth.tar
resume = False
branch = False
evaluate = False
tee_printing = None

