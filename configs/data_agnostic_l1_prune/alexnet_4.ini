[dataset]
dataset = cifar100
dataset_location = /data

[cnn]
architecture = alexnet
depth = 29
cardinality = 8
widen_factor = 4
growth_rate = 12
compression_rate = 2

[training_hyperparameters]
print_only = False
total_epochs = 150
train_batch = 128
test_batch = 128
learning_rate = 0.01
dropout_ratio = 0.5
gamma = 0.1
momentum = 0.9
weight_decay = 5e-4
momentum_schedule = 
lr_schedule = 0 0.001 10 -1 20 -1
train_val_split = 0.8

[pruning_hyperparameters]
sub_name = entire_dataset
sub_classes = 
logs = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/logs.json
get_gops = False
inference_gops = False
unpruned_test_acc = False
no_finetune_channels_pruned = False
prune_filters = True
finetune = True
static = True
retrain = False
pruning_perc = 25
finetune_budget = 30
prune_after = 0
this_layer_up = 0
prune_weights = False
metric = weights
fbs_pruning = False
fbs_finetune = False
unpruned_ratio = 1.0
unpruned_lb = 0.1
batch_lim = -1
iterative_pruning_increment = 10
iterative_pruning_epochs = 15
change_in_rank = False

[entropy_hyperparameters]
entropy = False
layers = 
channels = -1
num_batches = 320
entropy_global_pruning = False

[pytorch_parameters]
manual_seed = -1
data_loading_workers = 4
gpu_id = 3
checkpoint_path = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/cifar100/entire_dataset/data_agnostic_l1_prune
test_name = pp_25
pretrained = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/cifar100/baseline/2019-08-28-10-40-35/orig/61-model.pth.tar
resume = False
branch = False
evaluate = False
tee_printing = None

