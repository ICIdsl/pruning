[dataset]
dataset = cifar100
dataset_location = /data

[cnn]
architecture = alexnet
depth = 29
cardinality = 8
widen_factor = 4
growth_rate = 12
compression_rate = 2

[training_hyperparameters]
print_only = True
total_epochs = 150
train_batch = 128
test_batch = 128
learning_rate = 0.01
dropout_ratio = 0.5
gamma = 0.1
momentum = 0.9
weight_decay = 5e-4
momentum_schedule = 
lr_schedule = 0 0.0001 5 0.01 15 -1 25 -1
train_val_split = 0.8

[pruning_hyperparameters]
sub_name = entire_dataset
sub_classes = 
get_gops = False
inference_gops = False
logs = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/logs.json
unpruned_test_acc = True
no_finetune_channels_pruned = False
prune_filters = False
finetune = False
static = False
retrain = False
pruning_perc = 50
finetune_budget = 30
prune_after = 5
channels_pruned = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/cifar100/entire_dataset/l1_prune/pp_95/2019-11-20-00-17-54/orig/pruned_channels.json
this_layer_up = 0
prune_weights = False
metric = weights
fbs_pruning = False
fbs_finetune = False
unpruned_ratio = 1.0
unpruned_lb = 0.1
batch_lim = -1
iterative_pruning_increment = 10
iterative_pruning_epochs = 15
change_in_rank = False

[entropy_hyperparameters]
entropy = False
layers = 
channels = -1
num_batches = 320
entropy_global_pruning = False

[pytorch_parameters]
manual_seed = -1
data_loading_workers = 4
gpu_id = 2
checkpoint_path = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/cifar100/subset1/val_l1_prune
test_name = pp_50
pretrained = /home/ar4414/pytorch_training/src/ar4414/pruning/logs/alexnet/cifar100/baseline/2019-08-28-10-40-35/orig/61-model.pth.tar
resume = False
branch = False
evaluate = False
tee_printing = None

